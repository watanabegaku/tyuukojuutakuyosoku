# -*- coding: utf-8 -*-
"""Nisjka 中古マンション.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1asWQHotSHZVUlH6Bln7ZTFROQWBlUJYw
"""

!unzip /content/sample_data/train.zip

import glob
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error as mae

# 各ファイルデータの結合
files = glob.glob('/content/train/*.csv')
data_list = []
for file in files:
  data_list.append(pd.read_csv(file, index_col=0))

df = pd.concat(data_list)

df.head(3)

df.shape

"""###前処理　重複行や欠損値処理"""

#重複行確認
df.duplicated(keep=False).value_counts()

# 重複行削除
df1 = df.drop_duplicates()

#重複行確認
df1.duplicated(keep=False).value_counts()

df1.isnull().sum()

# 欠損値が多い列ごと削除
df2 = df1.drop(labels=['種類','市区町村名','地域','土地の形状','間口','延床面積（㎡）','前面道路：方位','前面道路：種類','前面道路：幅員（ｍ）'],axis=1)

df2.isnull().sum()

df2.dtypes

# object文の欠損値の行を削除  Bパターン
#df3 = df2.dropna(subset=['最寄駅：距離（分）','間取り','面積（㎡）','建築年'])

# 削除しない場合　Aパターン
df3= df2

df3.shape

#欠損値確認
df3.isnull().sum()

# 文字列、数値　確認
df3.dtypes

df3['建ぺい率（％）'].describe()

import matplotlib.pyplot as plt

plt.hist(df3['建ぺい率（％）'].dropna())

df3['容積率（％）'].describe()

plt.hist(df3['容積率（％）'].dropna())

#　欠損値を平均値で補完
df3 = df3.fillna({'建ぺい率（％）':df['建ぺい率（％）'].median(), '容積率（％）':df['容積率（％）'].median()})

df3.isnull().sum()

df3.to_csv('juutakumaesyori.csv')

"""###文字列を数値化する処理

"""

import glob
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error as mae
import matplotlib.pyplot as plt

df3 = pd.read_csv('/content/sample_data/juutakumaesyori.csv')
df3.head()

#カテゴリカル変数を含んだデータのみ抽出
df3_obj = df3.select_dtypes(include='object')
df3_obj.head(10)

#ユニーク値の数
df3_uni = df3_obj.nunique()
df3_uni

def data_pre(df3):
  nonnull_list = []
  for col in df3.columns:
    nonnull = df3[col].count()
    if nonnull == 0:
      nonnull_list.append(col)
  df3 = df3.drop(nonnull_list, axis=1)

  #df3 = df3.drop('市区町村名', axis=1)

  #df3 = df3.drop('種類', axis=1)

  dis = {
      "30分?60分":45,
      "1H?1H30":75,
      "2H?":120,
      "1H30?2H":105
  }
  df3['最寄駅：距離（分）'] = df3['最寄駅：距離（分）'].replace(dis).astype(float)

  df3['面積（㎡）'] = df3['面積（㎡）'].replace("2000㎡以上", 2000).astype(float)

  y_list = {}
  for i in df3['建築年'].value_counts().keys():
    if "平成" in i:
      num = float(i.split("平成")[1].split("年")[0])
      year = 33 - num
    if "令和" in i:
      num = float(i.split("令和")[1].split("年")[0])
      year = 3 - num
    if "昭和" in i:
      num = float(i.split("昭和")[1].split("年")[0])
      year = 96 - num
    y_list[i] = year
  y_list["戦前"] = 76
  df3['建築年'] = df3['建築年'].replace(y_list)

  year = {
      "年第1四半期": ".25",
      "年第2四半期": ".50",
      "年第3四半期": ".75",
      "年第4四半期": ".99"
  }
  year_list = {}
  for i in df3["取引時点"].value_counts().keys():
    for k, j in year.items():
      if k in i:
        year_rep = i.replace(k,j)
    year_list[i] = year_rep
  df3["取引時点"] = df3["取引時点"].replace(year_list).astype(float)

  for col in ['都道府県名', '地区名', '最寄駅：名称', '間取り', '建物の構造', '用途', '都市計画', '改装','今後の利用目的','取引の事情等']:
    df3[col] = df3[col].astype('category')

  return df3

df3 = data_pre(df3)

#データ形式確認
df3.dtypes

#欠損値確認
df3.isnull().sum()

df3['最寄駅：距離（分）'].describe()

plt.hist(df3['最寄駅：距離（分）'].dropna())

df3['建築年'].describe()

#　欠損値を平均値で補完 フロート関数変換後
df3 = df3.fillna({'最寄駅：距離（分）':df3['最寄駅：距離（分）'].median(), '建築年':df3['建築年'].median()})

df3.head(3)

!pip install lightgbm
!pip install optuna

#OptunaでLightGBMのハイパーパラメーターチューニング
#import optuna.integration.lightgbm as lgb_tune

df3_train, df3_val = train_test_split(df3, test_size=0.2)

col = "取引価格（総額）_log"
train_y = df3_train[col]
train_x = df3_train.drop(col, axis=1)

val_y = df3_val[col]
val_x = df3_val.drop(col, axis=1)

trains = lgb.Dataset(train_x, train_y)
valids = lgb.Dataset(val_x, val_y)

params = {
    "objective":"regression",
    "metrics":"mae"
}

#パラメーターチューニングを使う場合
#model_tune = lgb_tune.train(params, trains, valid_sets=valids, verbose_eval = 100, early_stopping_rounds=100)

#ノーマルの場合
model = lgb.train(params, trains, valid_sets=valids, num_boost_round = 600, early_stopping_rounds=100)

# ハイパーパラメーターのモデル構築
#model_tune.params
#model = lgb.train(model_tune.params, trains, valid_sets=valids, num_boost_round=700, early_stopping_rounds=100)

df_test = pd.read_csv('/content/sample_data/test.csv', index_col=0)
df_test = data_pre(df_test)

df_test

df_test.shape

df_test.isnull().sum()

df_test.dtypes

#　欠損値を平均値で補完 フロート関数変換後　Aパターン
df_test = df_test.fillna({'建ぺい率（％）':df3['建ぺい率（％）'].median(), '容積率（％）':df3['容積率（％）'].median()})
df_test = df_test.fillna({'最寄駅：距離（分）':df3['最寄駅：距離（分）'].median(), '建築年':df3['建築年'].median()})

df_test = df_test.drop('市区町村名', axis=1)
df_test = df_test.drop('種類', axis=1)

predict = model.predict(df_test)
df_test['取引価格（総額）_log'] = predict
df_test[['取引価格（総額）_log']].to_csv('submit_test.csv')

